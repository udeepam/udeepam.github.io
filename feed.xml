<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://udeepam.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://udeepam.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-17T18:20:44+00:00</updated><id>https://udeepam.github.io/feed.xml</id><title type="html">Udeepa Meepegama</title><subtitle>Udeepa Meepegama&apos;s personal website. Machine learning blog posts.</subtitle><entry><title type="html">Kullback-Leibler Divergence</title><link href="https://udeepam.github.io/blog/2024/kl-divergence/" rel="alternate" type="text/html" title="Kullback-Leibler Divergence"/><published>2024-10-18T00:00:00+00:00</published><updated>2024-10-18T00:00:00+00:00</updated><id>https://udeepam.github.io/blog/2024/kl-divergence</id><content type="html" xml:base="https://udeepam.github.io/blog/2024/kl-divergence/"><![CDATA[<h2 id="definition">Definition</h2> <hr/> <p>The Kullback-Leibler divergence (also known as the KL-divergence or relative entropy) is a measure of how one probability distribution is “different” from a second reference probability distribution.</p> <p>Let $p(x)$ be the true distribution and $q(x)$ be the approximating distribution for the random variable $X$. The forward KL-divergence from $q$ to $p$ is</p> \[\begin{equation} D_{\text{KL}}(p\mid\mid q)=\mathbb{E}_{p(x)}\bigg[\log\frac{p(x)}{q(x)}\bigg]=\mathbb{E}_{p(x)}\Big[\log p(x)-\log q(x)\Big]. \end{equation}\] <p>The reverse KL-divergence from $p$ to $q$ is</p> \[\begin{equation} D_{\text{KL}}(q\mid\mid p)=\mathbb{E}_{q(x)}\bigg[\log\frac{q(x)}{p(x)}\bigg]=\mathbb{E}_{q(x)}\Big[\log q(x)-\log p(x)\Big]. \end{equation}\] <h2 id="properties">Properties</h2> <hr/> <p>The KL-divergence has the following properties:</p> <ul> <li> It is not symmetric in its arguments and hence not a valid distance metric <div> $$D_{\text{KL}}(p \mid\mid q) \neq D_{\text{KL}}(q \mid\mid p).$$ </div> </li> <li> $D_{\text{KL}}(p \parallel q) \geq 0$, for all $p(x)$ and $q(x)$. <details> <summary>Show proof</summary> <p>We use Jensen's inequality for convex functions:</p> <div> $$ \mathbb{E}\big[f(x)\big]\leq f\big(\mathbb{E}[x]\big), $$ </div> <p>such that</p> <div> $$ \begin{align*} D_{\text{KL}}(q \parallel p) &amp;= \mathbb{E}_{q(x)} \bigg[-\log \frac{p(x)}{q(x)}\bigg] \\ &amp;\geq -\log \mathbb{E}_{q(x)} \bigg[\frac{p(x)}{q(x)}\bigg] \\ &amp;= -\log \sum_{x} p(x) \\ &amp;= -\log 1 \\ &amp;= 0 \end{align*} $$ </div> </details> </li> <li> $D_{\text{KL}}(p \mid\mid q) = 0$, if and only if $p(x) = q(x)$. </li> </ul> <h2 id="minimising-the-forward-kl">Minimising the Forward KL</h2> <hr/> <p>We have a choice of minimising either the forward KL or reverse KL<d-footnote>Something 1.</d-footnote> We have a choice of minimising either the forward KL or reverse KL<d-footnote>Something 2.</d-footnote></p> \[q^\star=\underset{q}{\text{argmin }}D_{\text{KL}}(p\parallel q) = \underset{q}{\text{argmin }}\mathbb{E}_{p(x)}\Big[\log p(x)-\log q(x)\Big]\] <p>mean-seeking. The expectation is w.r.t. $p(x)$ and so vanishes when $p(x)=0$. Thus to minimise $\log p(x)-\log q(x)$ when $p(x)&gt;0$ means putting $q(x)$ weight when $p(x)&gt;0$. When $p$ has multiple modes $q$ chooses to blur the modes together to approximate the mean.<d-cite key="gregor2015draw"></d-cite></p> <h2 id="minimising-the-reverse-kl">Minimising the Reverse KL</h2> <hr/> \[q^\star=\underset{q}{\text{argmin }}D_{\text{KL}}(q\parallel p)= \underset{q}{\text{argmin }}\mathbb{E}_{q(x)}\Big[\log q(x)-\log p(x)\Big]\] <p>mode-seeking. Forces $q(x)=0$ when $p(x)=0$ and hence makes it concentrate on one of the modes of $p(x)$.</p>]]></content><author><name></name></author><category term="kl-divergence"/><category term="jensen-inequality"/><summary type="html"><![CDATA[Definition]]></summary></entry></feed>