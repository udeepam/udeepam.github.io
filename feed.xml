<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://udeepam.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://udeepam.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-19T14:04:51+00:00</updated><id>https://udeepam.github.io/feed.xml</id><title type="html">Udeepa Meepegama</title><subtitle>Udeepa Meepegama&apos;s personal website. Machine learning blog posts.</subtitle><entry><title type="html">Distill</title><link href="https://udeepam.github.io/blog/2024/distill/" rel="alternate" type="text/html" title="Distill"/><published>2024-10-19T00:00:00+00:00</published><updated>2024-10-19T00:00:00+00:00</updated><id>https://udeepam.github.io/blog/2024/distill</id><content type="html" xml:base="https://udeepam.github.io/blog/2024/distill/"><![CDATA[<h2 id="definition">Definition</h2> <p>The Kullback-Leibler divergence (also known as the KL-divergence or relative entropy) is a measure of how one probability distribution is “different” from a second reference probability distribution.</p> <p>Let $p(x)$ be the true distribution and $q(x)$ be the approximating distribution for the random variable $X$. The forward KL-divergence from $q$ to $p$ is</p> \[D_{KL}(p\mid\mid q)=\mathbb{E}_{p(x)}\bigg[\log\frac{p(x)}{q(x)}\bigg]=\mathbb{E}_{p(x)}\Big[\log p(x)-\log q(x)\Big]\] <p>The reverse KL-divergence from $p$ to $q$ is</p> \[D_{KL}(q\mid\mid p)=\mathbb{E}_{q(x)}\bigg[\log\frac{q(x)}{p(x)}\bigg]=\mathbb{E}_{q(x)}\Big[\log q(x)-\log p(x)\Big]\] <p>The KL-divergence has the following properties:</p> <ul> <li> <p>Not symmetric in it’s arguments</p> \[D_{KL}(p\mid\mid q)\neq D_{KL}(q\mid\mid p)\] </li> <li> <p>$D_{KL}(p\parallel q)\geq0$, for all $p(x)$ and $q(x)$.</p> </li> <li> <p>$D_{KL}(p\mid\mid q)=0$, if and only if $p(x)=q(x)$.</p> </li> </ul> <details><summary>Click here to know more</summary> <p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p> </details>]]></content><author><name></name></author><category term="kl-divergence"/><category term="jensen-inequality"/><summary type="html"><![CDATA[Definition]]></summary></entry><entry><title type="html">Kullback-Leibler Divergence</title><link href="https://udeepam.github.io/blog/2024/kl-divergence/" rel="alternate" type="text/html" title="Kullback-Leibler Divergence"/><published>2024-10-18T00:00:00+00:00</published><updated>2024-10-18T00:00:00+00:00</updated><id>https://udeepam.github.io/blog/2024/kl-divergence</id><content type="html" xml:base="https://udeepam.github.io/blog/2024/kl-divergence/"><![CDATA[<h2 id="definition">Definition</h2> <p>The Kullback-Leibler divergence (also known as the KL-divergence or relative entropy) is a measure of how one probability distribution is “different” from a second reference probability distribution.</p> <p>Let $p(x)$ be the true distribution and $q(x)$ be the approximating distribution for the random variable $X$. The forward KL-divergence from $q$ to $p$ is</p> \[D_{KL}(p\mid\mid q)=\mathbb{E}_{p(x)}\bigg[\log\frac{p(x)}{q(x)}\bigg]=\mathbb{E}_{p(x)}\Big[\log p(x)-\log q(x)\Big]\] <p>The reverse KL-divergence from $p$ to $q$ is</p> \[D_{KL}(q\mid\mid p)=\mathbb{E}_{q(x)}\bigg[\log\frac{q(x)}{p(x)}\bigg]=\mathbb{E}_{q(x)}\Big[\log q(x)-\log p(x)\Big]\] <p>The KL-divergence has the following properties:</p> <ul> <li> <p>Not symmetric in it’s arguments</p> \[D_{KL}(p\mid\mid q)\neq D_{KL}(q\mid\mid p)\] </li> <li> <p>$D_{KL}(p\parallel q)\geq0$, for all $p(x)$ and $q(x)$.</p> </li> <li> <p>$D_{KL}(p\mid\mid q)=0$, if and only if $p(x)=q(x)$.</p> </li> </ul> <details><summary>Click here to know more</summary> <p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p> </details>]]></content><author><name></name></author><category term="kl-divergence"/><category term="jensen-inequality"/><summary type="html"><![CDATA[Definition]]></summary></entry></feed>